"use strict";(self.webpackChunkredkubes_github_io=self.webpackChunkredkubes_github_io||[]).push([[1477],{10:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"harbor-keycloak-istio","metadata":{"permalink":"/blog/harbor-keycloak-istio","source":"@site/blog/2021-04-13-harbor-keycloak-istio.md","title":"Integrating Harbor, KeyCloak and Istio","description":"This is our initial documentation setup for Otomi Container Platform. We hope you find it useful. Please don\'t hesitate to give us any feedback!","date":"2021-04-13T00:00:00.000Z","formattedDate":"April 13, 2021","tags":[{"label":"harbor","permalink":"/blog/tags/harbor"},{"label":"otomi","permalink":"/blog/tags/otomi"},{"label":"istio","permalink":"/blog/tags/istio"},{"label":"keycloak","permalink":"/blog/tags/keycloak"}],"readingTime":8.03,"truncated":false,"authors":[{"name":"Jehoszafat Zimnowoda","title":"Developer at Red Kubes","url":"https://github.com/j-zimnowoda","imageURL":"https://media-exp1.licdn.com/dms/image/C5603AQHsIqwpVlLrUQ/profile-displayphoto-shrink_400_400/0/1564471539392?e=1649289600&v=beta&t=q7c1IjhwLzQFPbhfxo5S8UCLzgtCKf_ogVjQwbMtwz0","key":"jzimnowoda"}],"frontMatter":{"slug":"harbor-keycloak-istio","title":"Integrating Harbor, KeyCloak and Istio","authors":"jzimnowoda","tags":["harbor","otomi","istio","keycloak"]},"nextItem":{"title":"Universal OPA Policies Development","permalink":"/blog/universal-opa-policy-development"}},"content":"This is our initial documentation setup for Otomi Container Platform. We hope you find it useful. Please don\'t hesitate to give us any [feedback](https://github.com/redkubes/redkubes.github.io/issues)!\\n\\nThis is my story is about building a multi-tenant Kubernetes environment that facilitates various DevOps teams (tenants) with their own Kubernetes namespace and private container registry (Harbor v2.1.0) with Single-Sign-On On (Keycloak v10.0.0) and service mesh (Istio 1.6.14) included.\\n\\n## Harbor, A Fat But Versatile Container Registry\\n\\nHarbor provides a container image registry, vulnerability scanning, container image signature and validation, OIDC based authentication and authorization. The fully-featured version is composed of ten micro-services. It is also CNCF graduated OSS.\\n\\nIn Harbor, a [project](https://goharbor.io/docs/2.2.0/working-with-projects/create-projects/) represents a container image registry, exposed under a unique URL, For example, **\\"harbor.otomi.io/team-demo/\\"**, where **team-demo** is a project name.\\n\\nBy creating projects you can achieve a multi-tenant container image repository for workloads in your Kubernetes cluster.\\n\\nIn Harbor, you can also define project membership, first by defining OIDC groups and then assigning them with a given project. For example, the OIDC group team-demo is a member of a team-demo project.\\n\\nNext, we want pods from a given Kubernetes namespace to pull container images from a private registry. The [Harbor robot accounts](https://goharbor.io/docs/1.10/working-with-projects/project-configuration/create-robot-accounts/) are made for that purpose. I recommend creating two robot accounts in each Harbor project. The first one for using Kubernetes as a [PullSecret](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/) at a given namespace and the second one for CI/CD pipeline.\\n\\n## Multi-Tenancy With Keycloak And Harbor\\n\\nKeycloak can be used as an identity provider. In Harbor, a user can log in by performing an [OIDC authentication code flow](https://openid.net/specs/openid-connect-core-1_0.html#CodeFlowAuth) with Keycloak. Upon successful authentication, a user is redirected back to Harbor with a [JSON Web Token](https://jwt.io/introduction) (JWT) signed by the identity Provider (Keycloak). The JWT contains a [ID token](https://openid.net/specs/openid-connect-core-1_0.html#IDToken).\\n\\nHarbor can verify JWT signature and automatically assign a user to a role and a project, based on groups claim from the ID token.\\n\\nThe following code snippet present an ID token with a groups claim:\\n\\n```json\\n{\\n  \\"iss\\": \\"https://keycloak.otomi.io/realms/master\\",\\n  \\"sub\\": \\"xyz\\",\\n  \\"name\\": \\"Joe Doe\\",\\n  \\"groups\\": [\\"team-dev\\", \\"team-demo\\"],\\n  \\"given_name\\": \\"Joe\\",\\n  \\"family_name\\": \\"Doe\\",\\n  \\"email\\": \\"joe.doe@otomi.io\\"\\n}\\n```\\n\\nThere is a \\"Joe Doe\\" user that belongs to team-dev and team-demo groups, which in Harbor can be matched to predefined OIDC groups. The ID token is issued by the Keycloak (iss property) that is running in the same Kubernetes cluster. Harbor can be configured to leverage ID tokens by specifying a set of authentication parameters.\\n\\nThere is an OIDC endpoint URL, which is matched against iss property from the ID token. Next, OIDC Client ID with OIDC client secret is used by Harbor to authenticate with a client at Keycloak. The group claim name is crucial for enabling Harbor OIDC group matching.\\n\\nIf you want to perform an automatic user onboarding process you should provide the following OIDC scopes: OpenID (iss and sub-properties) and email scope (email and email_verified properties).\\n\\nDo not forget about Keycloak, which requires an additional configuration of the OIDC client.\\n\\nThe client id is Otomi and the client secret is defined in the credentials tab. There are also valid redirect URLs and Web-origins that have to be set so a user can be redirected from and to the Harbor dashboard upon successful login.\\n\\n## Secure Connectivity With Istio Service Mesh\\n\\nIstio ensures service interconnectivity, encrypted traffic (mTLS), and routing (VirtualService + Gateways). Integrating Harbor with Istio is mostly about setting up proper URI routing.\\n\\nHarbor is composed of ten microservices:\\n\\n- Chartmuseum\\n- Clair\\n- Core\\n- Database\\n- Jobservice\\n- Portal\\n- Redis\\n- Registry\\n- Trivy\\n- Notary\\n\\nThe [Harbor Helm chart](https://github.com/goharbor/harbor-helm) provides also Nginx as a reverse server proxy service. You don\'t need it, instead, you can deploy the following Istio [VirtualService](https://istio.io/latest/docs/reference/config/networking/virtual-service/):\\n\\n```yaml\\napiVersion: networking.istio.io/v1beta1\\nkind: VirtualService\\nmetadata:\\n name: harbor\\nspec:\\n hosts:\\n \u2013 harbor.otomi.io\\n http:\\n   \u2013 match:\\n       \u2013 uri:\\n           prefix: \'/api/\'\\n       \u2013 uri:\\n           prefix: \'/c/\'\\n       \u2013 uri:\\n           prefix: \'/chartrepo/\'\\n       \u2013 uri:\\n           prefix: \'/service/\'\\n       \u2013 uri:\\n           prefix: \'/v1/\'\\n       \u2013 uri:\\n           prefix: \'/v2/\'\\n     route:\\n       \u2013 destination:\\n           host: harbor-harbor-core.harbor.svc.cluster.local\\n           port:\\n             number: 80\\n\\n   \u2013 match:\\n       \u2013 uri:\\n           prefix: /\\n     rewrite:\\n       uri: /\\n     route:\\n       \u2013 destination:\\n           host: harbor-harbor-portal.harbor.svc.cluster.local\\n           port:\\n             number: 80\\n```\\n\\nThe Virtual Services redirect URI paths into Harbor core service:\\n\\n- `/api/`\\n- `/c/`\\n- `/chartrepo/`\\n- `/service/`\\n- `/v1/`\\n- `/v2/`\\n\\nAll other URI paths are redirected to the Harbor portal service (dashboard).\\n\\nThe destination hosts from harbor VirtualService is a Fully Qualified Domain Name (FQDN) that indicates the Kubernetes namespace of the Harbor services. It makes it possible for the Istio Ingress gateway to route the incoming traffic.\\n\\nYou might have noticed that traffic is routed to port 80 (HTTP) instead of 433 (HTTPS). It is because I disabled Harbor internal TLS in favor of the Istio proxy sidecar that enforces mTLS for each Harbor service.\\n\\n## Automation With Otomi To Support Multi-Tenancy\\n\\nWith Otomi, we strive to integrate best of breed Open-Source projects and provide multi-tenancy awareness out-of-the-box.\\n\\nMulti-tenancy is challenging and requires configuration automation to ensure scalability.\\n\\nPart of Otomi\'s automation is to configure applications, so they are aware of each other. We do it either by using a declarative approach when that is possible, or else by interacting with their (REST) APIs directly.\\n\\nWe have generated REST API clients based on the open API specification for Harbor and Keycloak. You are welcome to use [our factory](https://github.com/redkubes/otomi-clients)\\n\\nNext, we implemented idempotent tasks that leverage these REST API clients and automate service configuration. These are run as Kubernetes jobs whenever a configuration changes.For Harbor, we have automated the creation of projects, OIDC groups, project membership, and OIDC settings.\\n\\nFor Keycloak, we have automated configuration of the external identity provider, group names normalization, deriving Client ID, Client Secret and more. Are you inspired about both? Then take a look at our open-source code.\\n\\n## Pitfalls\\n\\nEach OSS project has its own goals and milestones, thus it may be challenging to integrate various projects to work together. Here, I share just a few Issues that I stumbled upon.\\n\\n## Harbor\\n\\nThe container image registry, provided by Harbor, and Docker CLI do not support the OIDC protocol. Instead, it uses a username/password-based authentication. It means that whenever you perform Docker Login/push/pull commands, the HTTPS traffic from a docker client to the container registry does not carry JWT. Make sure to exclude /v1/, /v2/ and /service/ Harbor URI paths from the JWT verification. Otherwise, you won\'t be able to use the registry.\\n\\nNext, OIDC users may experience issues with their Docker credentials (CLI secrets) that suddenly are invalidated. The [CLI secret](https://goharbor.io/docs/1.10/administration/configure-authentication/oidc-auth/) depends on the validity of the ID token, which has nothing in common with the container registry. This hybrid security solution is something that a regular docker user does not expect and can be a source of many misunderstandings.\\n\\nFollow [this](https://github.com/goharbor/harbor/issues/14172) thread to get more insights.\\n\\nThe good news is that if you are an automation freak like me, you don\'t actually need CLI secrets. Instead, you can use [Harbor robot accounts](https://goharbor.io/docs/1.10/working-with-projects/project-configuration/create-robot-accounts/) that do not depend on OIDC authentication.\\n\\n## Keycloak Or Other Identity Provider\\n\\nIf your organization decides to migrate users to another identity provider you may experience a duplicated user error: \\"Conflict, the user with the same username or email has been onboarded\\".\\n\\nIt is because **sub** and/or **iss** scopes from ID token may change, so the same user trying to login to the harbor dashboard will be treated as a new one. The onboarding process starts but fails because Harbor requires each user to have a unique email address. I ended up removing existing OIDC users from Harbor and allowing them to onboard once again. Interestingly the community of Harbor users is having a broad debate about using OIDC protocol and could not agree on a final solution so far. I encourage you to take a look [here](https://github.com/goharbor/harbor/issues/14172) for a very insightful conversation about it.\\n\\n## Istio\\n\\nWhile making Harbor services part of the Istio service mesh, it is very important that Kubernetes services are using port names that follow the Istio convention. For example, the Harbor registry services should have an HTTP-registry port name, instead of a registry. See the following example:\\n\\n```yaml\\napiVersion: v1\\nkind: Service\\nmetadata:\\n name: harbor-harbor-registry\\nspec:\\n ports:\\n \u2013 name: http-registry\\n port: 5000\\n protocol: TCP\\n targetPort: 5000\\n \u2013 name: http-controller\\n port: 8080\\n protocol: TCP\\n targetPort: 8080\\n```\\n\\nIf the **service port name** does not follow the Istio convention, Harbor core service is not able to communicate with the Harbor registry service in Istio service mesh. Attempting to login into the Docker registry will end up with an \\"authentication required\\" error.\\n\\n## Takeaways\\n\\n- Harbor is a suitable solution for deploying a self-hosted container image repository in a multi-tenant Kubernetes cluster. Nevertheless, the lack of configuration automation makes it hard to maintain it in a constantly changing environment\\n- Using the OIDC group\'s claim can be used for granting users default role and access to Harbor project(s)\\n- While working with Istio, do not mess up with named ports\\n- Stay in touch with open-source communities for projects that you are using, as they are a true treasure trove of information\\n\\nI hope that this article provides you a good insight into more advanced Harbor integration in the Kubernetes cluster.\\n\\nPart of Otomi\'s automation is to configure applications, so they are aware of each other. We do it either by using a declarative approach when that is possible, or else by interacting with their (REST) APIs directly.\\n\\nWe have generated REST API clients based on the open API specification for Harbor and Keycloak. You are welcome to use our [factory](https://github.com/redkubes/otomi-clients) for building and publishing open API clients.\\n\\nThis article was originally posted by Jehoszafat Zimnowoda on [medium.com](https://jzimnowo.medium.com/harbor-keycloak-and-istio-a-good-dance-troupe-6c3520fb87de)."},{"id":"universal-opa-policy-development","metadata":{"permalink":"/blog/universal-opa-policy-development","source":"@site/blog/2021-03-11-universal-opa-policy-development.md","title":"Universal OPA Policies Development","description":"Introduction","date":"2021-03-11T00:00:00.000Z","formattedDate":"March 11, 2021","tags":[{"label":"otomi","permalink":"/blog/tags/otomi"},{"label":"opa","permalink":"/blog/tags/opa"},{"label":"gatekeeper","permalink":"/blog/tags/gatekeeper"},{"label":"rego","permalink":"/blog/tags/rego"},{"label":"security","permalink":"/blog/tags/security"}],"readingTime":9.58,"truncated":false,"authors":[{"name":"Alin Spinu","title":"Otomi contributor","url":"https://github.com/rawc0der","imageURL":"https://avatars.githubusercontent.com/u/1760897?v=4","key":"aspinu"}],"frontMatter":{"slug":"universal-opa-policy-development","title":"Universal OPA Policies Development","authors":"aspinu","tags":["otomi","opa","gatekeeper","rego","security"]},"prevItem":{"title":"Integrating Harbor, KeyCloak and Istio","permalink":"/blog/harbor-keycloak-istio"}},"content":"## Introduction\\n\\nThis is a story behind the trenches of writing **Rego** policies and how to unravel the cumbersome process of working with **Gatekeeper** vs **Conftest** for validating Kubernetes resources.\\n\\nWorking with policy compliant Kubernetes clusters is on the radar for a lot of companies these days, especially if you\'re walking the path towards popular certifications like ISO/IEC 27001 for Information Security Management.\\n\\n## Hands-on using OPA in Otomi Container Platform\\n\\nAs some of you probably already know, the Kubernetes native PodSecurityPolicy resource is going to be deprecated, see [Github](https://github.com/kubernetes/kubernetes/pull/97171) and [Google docs](https://docs.google.com/document/d/1VKqjUlpU888OYtIrBwidL43FOLhbmOD5tesYwmjzO4E/edit#)\u200a\u2014\u200athis leaves way for external projects like Open Policy Agent to be used as the new standard for developing and enforcing policy rules.\\n\\nOtomi is using OPA as the standard for providing policy enforcement because of the popularity and commitment to the Kubernetes community, ease of use and also for the future project development plans.\\n\\nOne of the key principles of Otomi Container Platform is that it is easy to use and provide clarity for integrations, allowing developers to easily extend any platform feature and provide integrated security for everything from the ground up.\\n\\n## OPA ecosystem common knowledge\\n\\nDecisions are handled by the means of **Admission Controllers** such as **OPA kube-mgmt** project or Gatekeeper, which I will touch upon in a minute, but also remember that we can validate things using **Rego query** language on any plain files using static analysis tools like Conftest. The list of supported Conftest formats include (but is not limited to): json, yaml, Dockerfile, INI files, XML, etc.Here\'s the problem, Conftest and Gatekeeper are on the path of diverging. Although they speak the same REGO language, the two disagree on some aspects.\\n\\n## In-Cluster vs Static Resources Policy Wars\\n\\nWorking in a policy constricted environment is like having \\"parental controls\\" turned on for unprivileged users, allowing administrators to decide what kind of resources and setup are safest for their flock of Kubernetes clusters. From an application developer\'s perspective, being denied access to deploy some resources means that they are not adhering to the rules imposed for that environment and should decide to find and fix the missing links in this setup.\\n\\nPolicy administrators/developers on the other hand, struggle with finding the correct enforcement strategies and adjusting policy parameters according to desired state or allowing certain exclusions for cases where policy enforcement does not make sense. For example: system namespaces, cloud vendor specific namespaces or anything that should avoid intervention by default. There is no golden rule for policy adoption and you are in charge of overcoming your own mistakes if something is not right.\\n\\nLet\'s start with the simple use-case of running policy checks against any kind of YAML resource. Then I move forward with more details about in-cluster Kubernetes admission review objects.\\n\\n## Conftest in action\\n\\nWith a simple command we can test if a helm chart is violating any rules in the provided policies folder.\\n\\n```bash\\n$ helm template stable/keycloak | conftest test --policy ./policies/ --all-namespaces -\\nFAIL - Policy: container-limits - container <keycloak> has no resource limits\\nFAIL - Policy: container-limits - container <keycloak-test> has no resource limits\\n162 tests, 160 passed, 0 warnings, 2 failures, 0 exceptions\\n```\\n\\nThe generated yaml files are streamed into Conftest and policies are tested one by one.\\n\\nBy examining the log message, we can see that the container-limits policy is marking two resources as failures. Now all we have to do is modify the templates to provide a \\"sensitive\\" amount of resource limits to the indicated containers and our policy checks will pass successfully! Hooray\\n\\nThis is pretty useful if you want to adopt new helm applications, but don\'t want to deploy anything to the cluster unless it\'s well-examined for any violations. Conftest supports passing values to the policies using the \u2013data option, which allows policy designers to configure different settings through parameters. Parameters can help us control any aspect of creating configurable rules for resources. I will return to that in a moment.\\n\\n## Running Gatekeeper\\n\\nGatekeeper is becoming the new standard for implementing security policies in Kubernetes, endorsing a broad ecosystem to spread ideas. Enforcing policy decisions works by using a validating web-hook, intercepting any request authenticated by the api-server and checking if the request meets the defined policy\u200a or rejecting it otherwise.\\n\\nTrying to create a non-conformant resource object in a Gatekeeper enabled cluster results in an error with a message to explain the rejected request.\\n\\n```bash\\n$ helm template charts/redis | kubectl apply -f -\\n\\nsecret/redis created\\nconfigmap/redis created\\nservice/redis-master created\\nError from server (\\n[denied by banned-image-tags] Policy: banned-image-tags - container <redis> has banned image tag <latest>, banned tags are {\\"latest\\", \\"master\\"}\\n[denied by psp-allowed-users] Policy: psp-allowed-users - Container redis is attempting to run as disallowed user 0. Allowed runAsUser: {\\"rule\\": \\"MustRunAsNonRoot\\"}\\n)\\nerror when creating \\"redis/templates/redis-master-statefulset.yaml\\": admission webhook \\"validation.gatekeeper.sh\\" denied the request\\n```\\n\\nAs you can see, some of the resources get created, but the command fails with a denial from the admission webhook.\\n\\nTo make this resource valid, small tweaks to the image tag and securityContext fields will do the trick.\\n\\n## Policy Development in Gatekeeper context\\n\\nSimilar to plain policy files which Conftest uses, Gatekeeper policy library works by loading into memory a collection of wrapped rego files in Kubernetes CRDs called ConstraintTemplates.\\n\\nTo enforce a policy for certain resource types we need to instantiate a Constraint (CR resource) which is acting like the blueprint with desired values or parameters.\\n\\nExample Gatekeeper setup for Config and Constraint resources:\\n\\n```yaml\\n--- # Gatekeeper Config\\napiVersion: config.gatekeeper.sh/v1alpha1\\nkind: Config\\nmetadata:\\n  name: config\\n  namespace: gatekeeper-system\\nspec:\\n  match:\\n    - excludedNamespaces:\\n        - gatekeeper-system\\n        - kube-system\\n      processes:\\n        - \'*\'\\nsync:\\n  syncOnly:\\n    - group: \'\'\\n      kind: Pod\\n      version: v1\\n--- # ContainerLimits Constraint\\napiVersion: constraints.gatekeeper.sh/v1beta1\\nkind: ContainerLimits\\nmetadata:\\n  name: containerlimits\\nspec:\\n  match:\\n    kinds:\\n      - apiGroups:\\n          - apps\\n          - \'\'\\n        kinds:\\n          - DaemonSet\\n          - Deployment\\n          - StatefulSet\\n          - Pod\\nparameters:\\n  container-limits:\\n    enabled: false\\n    cpu: \'2\'\\n    memory: 2000Mi\\n```\\n\\nSo far this looks pretty easy\u200a. We decide to enable this policy for all namespaces except for \\"gatekeeper-system\\" & \\"kube-system\\" and Gatekeeper will test all our containers for resource limits.\\n\\nHold on.. Where is the definition for this policy? Rego Rules are defined in CRD files called ConstraintTemplates and they need to be deployed prior to the Constraint instance.\\n\\n```yaml\\n--- # ConstraintTemplate for container-limits policy\\napiVersion: templates.gatekeeper.sh/v1beta1\\nkind: ConstraintTemplate\\nmetadata:\\n name: containerlimits\\nspec:\\n crd:\\n   spec:\\n     names:\\n       kind: ContainerLimits\\n     validation:        # Schema for the `parameters` field\\n       openAPIV3Schema:\\n...\\ntargets:\\n- target: admission.k8s.gatekeeper.sh\\n libs:\\n   - |-\\n     package lib.utils\\n     ...\\n rego: |-\\n   package containerlimits\\n   ...\\n```\\n\\nTo simplify the example, we\'ve cut out parts of our file, but you can also view it on the [Gatekeeper Library repo](https://github.com/open-policy-agent/gatekeeper-library/blob/master/library/general/containerlimits/template.yaml). The \\"rego\\" property is where the actual policy definition lives and any dependency libraries can be declared in the list of \\"libs\\". We won\'t go into how Rego rules for violations work now, but you can enjoy all the fun of learning a powerful query language inspired by decades old [Datalog](https://www.openpolicyagent.org/docs/latest/policy-language/).\\n\\n## Planning for Unification\\n\\nWhile the process of creating new policies seems to bear a lot of weight, luckily there is a simple CLI tool to speed up our work called Konstraint.\\n\\nYou can define the rules in Rego files and Konstraint will wrap our policy and all dependency libraries in the necessary CRD files for Gatekeeper. Konstraint has core library functions for working with all kinds of kubernetes native objects which makes it an indispensable tool for Rego development. OK, so this means we only care about writing Rego files and we can test them in both Gatekeeper and Conftest.\\n\\n## Open Issues in OPA Community\\n\\nWhile that is true, after delving deeper into the bowels of Gatekeeper and Conftest, we found conflicting design concepts which completely ruin this unified policy development mindset.\\n\\nThe way you can define parameters in two libraries is different, and Gatekeeper has a restrictive parser which does not allow arbitrary imports in the rego definition. There has been an attempt to change this hard limitation in a discussion [here](https://github.com/open-policy-agent/gatekeeper/issues/1046\\n\\n## Current State of Unified Rego\\n\\nTo echo the subject once again, we are interested in reducing the boilerplate and simplify the process of developing Policies.\\n\\n`[ Same Rego == different contexts ]`\\n\\nWorking with one common set Rego policy files and using the same source code for testing \\"static files\\", generated from CI Builds or across any \\"Gatekeeping context\\", where objects are created via the Kubernetes api. Let\'s cut to the chase, and say we already made this happen, and delivered a working solution in Otomi. By using the available community tools, lots of integration work and a lot of customization additions. We now have a rich collection of policies and utility functions defined for our Kubernetes clusters\u200a. You can browse them in the [Otomi-core repository](https://github.com/redkubes/otomi-core/tree/master/policies/).\\n\\n## How Istio is Mutating Objects in the Background\\n\\nSo by now, we understand all the nitty gritty details about Rego policies and Gatekeeper\u200a\u2014\u200abut there will always be external factors, changing the state of the world and you can find yourself in a closed box situation, nowhere to go.\\n\\nThis situation becomes a nightmare when using Istio mesh for networking. In reality Istio is creating a \\"subspace\\" of resources by injecting a Sidecar container to all the pods in namespaces where service mesh communication is enabled. This kind of container is sometimes interfering with the security constraints design strategy.\\n\\n## Otomi Policy Features\\n\\nTo create some flexibility, we have further extended the policy exceptions capabilities by examining granular annotation information for every resource under analysis.\\n\\nComing back to the Parameters idea described a while back, if Policy files can read resource files as raw input, why not allow certain exceptions through annotating which resources we want to skip checking, similar to the excludeNamespace option for Gatekeeper.\\n\\nRego has a rich built-in library system and is a powerful language, allowing us to easily create robust utility functions for this design.\\n\\nTo give an example, using the following annotations will allow entire pod or certain containers from the pod to exclude one or more policies:\\n\\n```yaml\\n# Annotation for entire pod\\npolicy.otomi.io/ignore: psp-allowed-repos\\n# Annotation for Istio sidecar based containers\\npolicy.otomi.io/ignore-sidecar: psp-allowed-users,psp-capabilities\\n# Annotation for specific container (name=app-container)\\npolicy.otomi.io/ignore/app-container: banned-image-tags\\nAnother example of exclusion is to turn of the policy entirely by disabling it from the baseline settings:\\n# baseline configuration\\npolicies:\\n container-limits:\\n   enabled: false\\n   cpu: \'2\'\\n   memory: 2Gi\\n banned-image-tags:\\n   enabled: false\\n   tags:\\n     - latest\\n     - master\\n psp-host-filesystem:\\n   enabled: true\\n   allowedHostPaths:\\n     - pathPrefix: /tmp/\\n       readOnly: false\\n psp-allowed-users:\\n   enabled: true\\n   runAsUser:\\n     rule: MustRunAsNonRoot\\n   runAsGroup:\\n     rule: MayRunAs\\n     ranges:\\n       - min: 1\\n         max: 65535\\n   supplementalGroups:\\n     rule: MayRunAs\\n     ranges:\\n       - min: 1\\n         max: 65535\\n   fsGroup:\\n     rule: MayRunAs\\n     ranges:\\n       - min: 1\\n         max: 65535\\n psp-host-security:\\n   enabled: true\\npsp-host-networking-ports:\\n   enabled: true\\npsp-privileged:\\n   enabled: true\\npsp-capabilities:\\n   enabled: true\\n   allowedCapabilities:\\n     - NET_BIND_SERVICE\\n     - NET_RAW\\n psp-forbidden-sysctls:\\n   enabled: true\\n   forbiddenSysctls:\\n     - kernel.*\\n     - net.*\\n     - abi.*\\n     - fs.*\\n     - sunrpc.*\\n     - user.*\\n     - vm.*\\n psp-apparmor:\\n   enabled: true\\n   allowedProfiles:\\n     - runtime/default\\n     - docker/default\\npsp-selinux:\\n   enabled: false\\n   seLinuxContext: RunAsAny\\npsp-seccomp:\\n   enabled: false\\n   allowedProfiles:\\n     - runtime/default\\npsp-allowed-repos:\\n   enabled: true\\n   repos:\\n     - docker.io\\n     - gke.otomi.cloud\\n     - aks.otomi.cloud\\n     - eks.otomi.cloud\\n```\\n\\n## Conclusion\\n\\nTo pin down the state of the landscape\u200a\u2014\u200a designing Kubernetes policies is still evolving towards new heights and I can image in the near future more and more projects sharing the same policies from a registry like the [policy-hub](https://github.com/policy-hub/policy-hub-cli).\\n\\nWe think creating a common understanding about unified Rego is the key to a sunny future.\\n\\nThis article was originally posted by Alin Spinu on [Medium](https://spinualin.medium.com/universal-opa-policies-development-a3f88226e3d5)."}]}')}}]);